#!/usr/bin/env bash

set -e

PICK="4K0G0150"

echo ""
echo "PICK=${PICK}"
echo ""


die() { echo "$@" 1>&2 ; exit 1; }

# Directory to checkpoint generated by finetune_mobilnet_v1_on_munich.sh
PRETRAINED_CHECKPOINT_DIR="/home/zf/opt/drone-scalable-search/processed_dataset/munich/mobilenet_train/logs/all"

MODEL_NAME=mobilenet_v1
BATCH_SIZE=32

TRAIN_LAST_LAYER_STEPS=100
TRAIN_ALL_LAYER_STEPS=20000

JIT_DATASET_DIR="/home/zf/opt/drone-scalable-search/processed_dataset/munich/mobilenet_jit_train_${PICK}"
JIT_TRAIN_DIR="${JIT_DATASET_DIR}/logs"

if [[ -d "${JIT_TRAIN_DIR}" ]]; then
    die "${JIT_TRAIN_DIR} already exists."
fi
mkdir ${JIT_TRAIN_DIR}

echo ""
echo "Evaluate validation accuracy before jit training."
echo ""

python eval_image_classifier.py \
  --checkpoint_path=${PRETRAINED_CHECKPOINT_DIR} \
  --eval_dir=${JIT_TRAIN_DIR}/before_jit_train \
  --dataset_name=munich \
  --dataset_split_name=validation \
  --dataset_dir=${JIT_DATASET_DIR} \
  --model_name=${MODEL_NAME} 2>&1| tee ${JIT_TRAIN_DIR}/eval_before_jit_train_log.txt

sleep 5

# Reuse following scripts from finetune_mobilenet_v1_on_munich.sh
DATASET_DIR=${JIT_DATASET_DIR}
TRAIN_DIR=${JIT_TRAIN_DIR}

if [[ -n "`pgrep -u $USER -f 'python train_image_classifier'`" ]]; then
    die "User $USR is already running python train_image_classifier.py. YOU SHALL NOT PASS."
fi

echo ""
echo "Start training laster layer in background: ${DATASET_DIR}"
echo ""

# Fine-tune only the new layers for TRAIN_LAST_LAYER_STEPS steps.
(python train_image_classifier.py \
  --train_dir=${TRAIN_DIR}/last_layer \
  --dataset_name=munich \
  --dataset_split_name=train \
  --dataset_dir=${DATASET_DIR} \
  --model_name=${MODEL_NAME} \
  --checkpoint_path=${PRETRAINED_CHECKPOINT_DIR} \
  --trainable_scopes=MobilenetV1/Logits \
  --max_number_of_steps=${TRAIN_LAST_LAYER_STEPS} \
  --batch_size=${BATCH_SIZE} \
  --learning_rate=0.01 \
  --learning_rate_decay_type=fixed \
  --save_interval_secs=60 \
  --save_summaries_secs=60 \
  --log_every_n_steps=10 \
  --optimizer=rmsprop \
  --weight_decay=0.00004 2>&1| tee ${TRAIN_DIR}/train_last_layer.log) &

echo ""
echo "Start evaluation loop on ${DATASET_DIR}"
echo ""

sleep 10s; python eval_image_classifier_loop.py \
  --checkpoint_path=${TRAIN_DIR}/last_layer \
  --eval_dir=${TRAIN_DIR}/last_layer \
  --dataset_name=munich \
  --dataset_split_name=validation \
  --dataset_dir=${DATASET_DIR} \
  --batch_size=4 \
  --eval_interval_secs=60 \
  --timeout=15 \
  --model_name=${MODEL_NAME} 2>&1| tee ${TRAIN_DIR}/eval_last_layer.log


if [[ -n "`pgrep -u $USER -f 'python train_image_classifier'`" ]]; then
    die "User $USR is already running python train_image_classifier.py. YOU SHALL NOT PASS."
fi

echo ""
echo "Start training all layers in background: ${DATASET_DIR}"
echo ""

# Fine-tune all layers for TRAIN_ALL_LAYER_STEPS steps.
(python train_image_classifier.py \
  --train_dir=${TRAIN_DIR}/all \
  --dataset_name=munich \
  --dataset_split_name=train \
  --dataset_dir=${DATASET_DIR} \
  --model_name=${MODEL_NAME} \
  --checkpoint_path=${TRAIN_DIR}/last_layer \
  --max_number_of_steps=${TRAIN_ALL_LAYER_STEPS} \
  --batch_size=${BATCH_SIZE} \
  --learning_rate=0.0001 \
  --learning_rate_decay_type=fixed \
  --save_interval_secs=60 \
  --save_summaries_secs=60 \
  --log_every_n_steps=10 \
  --optimizer=rmsprop \
  --weight_decay=0.00004 2>&1| tee ${TRAIN_DIR}/train_all_layer.log) &

echo ""
echo "Start evaluation loop on ${DATASET_DIR}"
echo ""

sleep 10s; python eval_image_classifier_loop.py \
  --checkpoint_path=${TRAIN_DIR}/all \
  --eval_dir=${TRAIN_DIR}/all \
  --dataset_name=munich \
  --dataset_split_name=validation \
  --dataset_dir=${DATASET_DIR} \
  --batch_size=4 \
  --eval_interval_secs=60 \
  --timeout=30 \
  --model_name=${MODEL_NAME} 2>&1| tee ${TRAIN_DIR}/eval_all_layer.log
